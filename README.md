# ğŸš€ Advanced RAG Pipeline - PDF Q&A System

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A production-ready **Retrieval-Augmented Generation (RAG)** system with a professional PyQt5 GUI for intelligent PDF document Q&A. Features semantic search, cross-encoder reranking, streaming responses, and support for multiple LLM models via Poe API.

![RAG Pipeline Demo](docs/demo.gif)

---

## âœ¨ Key Features

### ğŸ¯ Core Capabilities
- **ğŸ¤– Multi-Model Support**: Choose from 12+ LLMs (GPT-4o, Claude 3.5, Gemini, Llama, etc.)
- **ğŸ“„ Drag-and-Drop PDF Upload**: Intuitive GUI with file management
- **ğŸ” Semantic Search**: Sentence Transformers embeddings (384-dim, local inference)
- **ğŸ¯ Two-Stage Retrieval**: Initial semantic search â†’ Cross-encoder reranking
- **âš¡ Streaming Responses**: Real-time token-by-token output
- **ğŸ’¾ Intelligent Caching**: Persistent embedding cache (3-5x speedup on repeat queries)
- **ğŸ“š Multi-PDF Support**: Process and query multiple documents simultaneously
- **ğŸ¨ Professional GUI**: Modern PyQt5 interface with dark mode support

### ğŸ› ï¸ Technical Stack
- **Embeddings**: `all-MiniLM-L6-v2` (local, fast, 384-dim)
- **Reranker**: `ms-marco-MiniLM-L-12-v2` (cross-encoder)
- **LLM API**: Poe API (OpenAI-compatible)
- **PDF Processing**: pdfplumber with intelligent chunking
- **GUI**: PyQt5 with custom drag-drop widgets

---

## ğŸ“Š System Architecture

```mermaid
flowchart TB
    subgraph "User Interface"
        GUI["PyQt5 GUI<br/>Drag & Drop PDFs<br/>Model Selection"]
        CLI["CLI Interface<br/>Command Line<br/>Batch Processing"]
    end
    
    subgraph "Document Processing"
        PDF["PDF Loader<br/>pdfplumber"]
        CHUNK["Text Chunker<br/>500 words + overlap"]
        PDF --> CHUNK
    end
    
    subgraph "Retrieval Pipeline"
        EMB["Embedding Cache<br/>Sentence Transformers<br/>all-MiniLM-L6-v2"]
        SEARCH["Semantic Search<br/>Cosine Similarity<br/>Top-5 candidates"]
        RERANK["Reranker<br/>Cross-Encoder<br/>ms-marco-MiniLM"]
        
        SEARCH --> RERANK
    end
    
    subgraph "LLM Generation"
        POE["Poe API Client<br/>12+ Models<br/>Streaming Support"]
        PROMPT["Prompt Builder<br/>Context + Citations"]
        
        PROMPT --> POE
    end
    
    GUI & CLI --> PDF
    CHUNK --> EMB
    EMB --> SEARCH
    RERANK --> PROMPT
    POE --> GUI
    POE --> CLI
    
    style GUI fill:#4CAF50,stroke:#2E7D32,color:#fff
    style CLI fill:#4CAF50,stroke:#2E7D32,color:#fff
    style EMB fill:#2196F3,stroke:#1565C0,color:#fff
    style RERANK fill:#FF9800,stroke:#E65100,color:#fff
    style POE fill:#9C27B0,stroke:#6A1B9A,color:#fff
```

---

## ğŸ“‹ Table of Contents

- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
  - [GUI Mode](#gui-mode)
  - [CLI Mode](#cli-mode)
- [Configuration](#-configuration)
- [Architecture Details](#-architecture-details)
- [API Reference](#-api-reference)
- [Development](#-development)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš€ Installation

### Prerequisites
- Python 3.11 or higher
- 2GB RAM minimum (4GB recommended for multiple PDFs)
- Internet connection (first run downloads ML models ~100MB)
- **Windows Users**: [Microsoft Visual C++ Redistributable](https://aka.ms/vs/17/release/vc_redist.x64.exe) (required for PyTorch)

### Option 1: Using `uv` (Recommended)

```powershell
# Windows: First install Visual C++ Redistributable if you haven't
# Download: https://aka.ms/vs/17/release/vc_redist.x64.exe

# Clone the repository
git clone https://github.com/yourusername/rag-pipeline.git
cd rag-pipeline

# Install uv if not already installed
pip install uv

# Install dependencies
uv sync
```

### Option 2: Using `pip`

```powershell
git clone https://github.com/yourusername/rag-pipeline.git
cd rag-pipeline
pip install -e .
```

### Set API Key

Get your Poe API key from [poe.com](https://poe.com) and set it:

```powershell
# Windows PowerShell
$env:POE_API_KEY = "your-api-key-here"

# Linux/macOS
export POE_API_KEY="your-api-key-here"
```

---

## âš¡ Quick Start

### Launch GUI

```powershell
python gui_main.py
```

Or if installed with scripts:
```powershell
rag-gui
```

### Quick CLI Test

```powershell
python cli_main.py
```

This will:
1. Load `test-pdf.pdf` (if present)
2. Generate embeddings (cached for next time)
3. Run a demo query
4. Stream the response with citations

---

## ğŸ’» Usage

### GUI Mode

#### 1. **Configure Model**
   - Select your preferred LLM from the dropdown (default: `gpt-4o-mini`)
   - Check API key status indicator (âœ… green = ready)

#### 2. **Upload PDFs**
   - **Drag & Drop**: Drag PDF files into the drop zone
   - **Click Add**: Use "â• Add Files" button to browse
   - **Multi-select**: Select multiple files at once

#### 3. **Process Documents**
   - Click "âš¡ Process PDFs"
   - Wait for embedding generation (first time only)
   - Status shows: `âœ… Processed X PDF(s) - Y chunks ready`

#### 4. **Ask Questions**
   - Type your question in the text box
   - Click "ğŸš€ Ask Question"
   - Watch the streaming response appear in real-time
   - View source citations at the bottom

#### Example Questions:
- *"Summarize the main points from the documents"*
- *"What does the document say about [topic]?"*
- *"æ ¹æ“š PDF èªªæ˜é‡é»ä¸¦é™„å‡ºè™•"* (Chinese: Explain key points with sources)

---

### CLI Mode

#### Basic Usage

```powershell
python cli_main.py
```

#### Custom PDF and Query

```powershell
python cli_main.py --pdf=document1.pdf,document2.pdf --query="Your question here"
```

#### Example Output

```
ğŸš€ Advanced RAG Pipeline with Embeddings, Reranking & Streaming
======================================================================

ğŸ“„ Loading 1 PDF(s)...
âœ“ Extracted 28 chunks total

â“ Query: Explain the main points from the PDF with sources.

ğŸ” Retrieving relevant documents...
âœ“ Retrieved 3 relevant chunks

ğŸ’¬ Querying Poe API (streaming)...

======================================================================
RESPONSE:
======================================================================
Based on the PDF documents, the main points are:

1. [Content streams here in real-time...]

======================================================================
ğŸ“š SOURCE CITATIONS:
======================================================================
  [1] document.pdf, Page 3
  [2] document.pdf, Page 7
  [3] document.pdf, Page 12
======================================================================
```

---

## âš™ï¸ Configuration

### Model Selection (GUI)

Available models in dropdown:
- **GPT Models**: `gpt-4o-mini`, `gpt-4o`, `gpt-3.5-turbo`
- **Claude**: `claude-3.5-sonnet`, `claude-3-opus`, `claude-3-haiku`
- **Gemini**: `gemini-2.0-flash-exp`, `gemini-pro`
- **Llama**: `llama-3.3-70b`, `llama-3.1-405b`
- **Others**: `mistral-large`, `qwen-2.5-72b`

### Advanced Configuration (`src/config.py`)

```python
# RAG Parameters
CHUNK_SIZE = 500           # Words per chunk
CHUNK_OVERLAP = 50         # Overlap for context continuity
RETRIEVAL_TOP_K = 5        # Initial semantic search candidates
FINAL_TOP_K = 3            # Final results after reranking

# Embedding Configuration
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # Sentence Transformer model
EMBEDDING_CACHE_DIR = ".embeddings_cache"  # Cache location

# Reranking
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-12-v2"

# Streaming
STREAM_ENABLED = True      # Enable token-by-token streaming
```

---

## ğŸ—ï¸ Architecture Details

### Data Flow Diagram

```mermaid
sequenceDiagram
    participant User
    participant GUI
    participant PDFProcessor
    participant EmbeddingCache
    participant Reranker
    participant PoeAPI

    User->>GUI: Upload PDFs & Ask Question
    GUI->>PDFProcessor: Load & Chunk PDFs
    PDFProcessor-->>GUI: Return text chunks
    
    GUI->>EmbeddingCache: Get embeddings (with cache)
    EmbeddingCache->>EmbeddingCache: Check disk cache
    alt Cache Miss
        EmbeddingCache->>EmbeddingCache: Compute embeddings
        EmbeddingCache->>EmbeddingCache: Save to disk
    end
    EmbeddingCache-->>GUI: Return embeddings
    
    GUI->>EmbeddingCache: Embed user query
    EmbeddingCache-->>GUI: Query embedding
    
    GUI->>GUI: Semantic search (cosine similarity)
    GUI->>Reranker: Rerank top-5 candidates
    Reranker-->>GUI: Top-3 reranked results
    
    GUI->>GUI: Build RAG prompt with context
    GUI->>PoeAPI: Stream completion request
    
    loop Streaming
        PoeAPI-->>GUI: Token chunk
        GUI-->>User: Display token
    end
    
    GUI-->>User: Show citations
```

### Project Structure

```
rag-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”œâ”€â”€ core/                  # Core RAG components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py   # PDF loading & chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # Embedding generation & caching
â”‚   â”‚   â”œâ”€â”€ reranker.py        # Cross-encoder reranking
â”‚   â”‚   â””â”€â”€ retrieval.py       # Semantic search & prompt building
â”‚   â””â”€â”€ gui/                   # GUI components
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main_window.py     # Main application window
â”‚       â””â”€â”€ widgets.py         # Custom drag-drop widget
â”œâ”€â”€ tests/                     # Test suite
â”‚   â”œâ”€â”€ test_advanced.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ architecture.md
â”œâ”€â”€ examples/                  # Usage examples
â”œâ”€â”€ gui_main.py               # GUI entry point
â”œâ”€â”€ cli_main.py               # CLI entry point
â”œâ”€â”€ pyproject.toml            # Project metadata
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ .gitignore
```

---

## ğŸ“– API Reference

### Core Components

#### `EmbeddingCache`

```python
from src.core import EmbeddingCache

cache = EmbeddingCache()

# Generate embeddings for chunks
embeddings = cache.get_embeddings(chunks)  # Dict[str, np.ndarray]

# Embed a single query
query_emb = cache.embed_text("What is RAG?")  # np.ndarray (384,)
```

#### `Reranker`

```python
from src.core import Reranker

reranker = Reranker()

# Rerank candidates
results = reranker.rerank(
    query="What is RAG?",
    chunks=candidates,
    top_k=3
)  # List[Dict] with 'score' field
```

#### `retrieve_with_reranking`

```python
from src.core import retrieve_with_reranking

results = retrieve_with_reranking(
    query="Your question",
    chunks=all_chunks,
    embedding_cache=cache,
    reranker=reranker,
    top_k=3
)
```

---

## ğŸ› ï¸ Development

### Running Tests

```powershell
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_advanced.py

# With coverage
pytest --cov=src tests/
```

### Code Formatting

```powershell
# Format code
black src/ tests/

# Check types
mypy src/

# Lint
flake8 src/
```

### Building Documentation

```powershell
cd docs
make html
```

---

## ğŸ› Troubleshooting

### Issue: Windows DLL Error (PyTorch c10.dll)

**Error Message:**
```
OSError: [WinError 1114] Error loading "c10.dll" or one of its dependencies.
```

**Solution:**
Install Microsoft Visual C++ Redistributable:
1. Download: [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe)
2. Run the installer
3. Restart your terminal
4. Try running the application again

This is required for PyTorch to work on Windows.

### Issue: "POE_API_KEY not set"

**Solution:**
```powershell
$env:POE_API_KEY = "your-key"
```

Make it permanent by adding to PowerShell profile:
```powershell
notepad $PROFILE
# Add: $env:POE_API_KEY = "your-key"
```

### Issue: "Import PyQt5 could not be resolved"

**Solution:**
```powershell
pip install PyQt5
# Or with uv:
uv pip install PyQt5
```

### Issue: Slow first run

**Expected**: First run downloads ML models (~100MB). Subsequent runs use cache and are 3-5x faster.

### Issue: Out of memory with large PDFs

**Solution:** Reduce `CHUNK_SIZE` or `RETRIEVAL_TOP_K` in `src/config.py`

---

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes
4. Run tests: `pytest tests/`
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Sentence Transformers**: For excellent embedding models
- **OpenAI/Poe**: For powerful LLM APIs
- **PyQt5**: For robust GUI framework
- **pdfplumber**: For reliable PDF text extraction

---

## ğŸ“ Contact

- **Author**: Your Name
- **Email**: your.email@example.com
- **GitHub**: [@yourusername](https://github.com/yourusername)
- **Issues**: [GitHub Issues](https://github.com/yourusername/rag-pipeline/issues)

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/rag-pipeline&type=Date)](https://star-history.com/#yourusername/rag-pipeline&Date)

---

<div align="center">

**Made with â¤ï¸ by the RAG Pipeline Team**

[Report Bug](https://github.com/yourusername/rag-pipeline/issues) Â· [Request Feature](https://github.com/yourusername/rag-pipeline/issues) Â· [Documentation](https://github.com/yourusername/rag-pipeline/wiki)

</div>
